---
title: "Model 2: Decision Tree"
author: Luca Baggi
---

# Load libraries

```{r, message=FALSE}
library(tidymodels)
library(tidyverse)

# for downsampling
library(themis)

# for visualising importance of predictors
library(vip)
```
Also, we will need knowing the number of cores for parallelising the workload of tuning:

```{r}
cores <- parallel::detectCores()
cores
```


# Data and Final Feature Engineering

```{r, message=FALSE}
url = 'https://raw.githubusercontent.com/baggiponte/escooters-louisville-python/main/data/escooters_od.csv'

trips <-
  # read the data
  read_csv(url, col_types = cols(
    StartTime = col_datetime(format = '%Y-%m-%d %H:%M:%S')
  )) %>%
  # remove columns not involved in predictions
  select(-TripID, -EndTime) %>%
  # remove the outliers
  filter(
    # Duration between 0 and 30 minutes
    Duration > 0 & Duration <= 30 & # NOTE THE `&`
      # Distance between 0 and 5km
      Distance > 0 & Distance <= 5000
  ) %>%
  # manipulate cols:
  mutate(
    # turn into factors
    StartNH = as.factor(StartNH),
    EndNH = as.factor(EndNH),
    # first covid death reported is on March 21st, 2020
    Covid = as.factor(ifelse(StartTime > '2020-03-20 23:59:59', 1, 0))
  ) %>%
  # reduce the number of levels in the factor features
  mutate(
    # select those with p > 0.01
    StartNH = fct_lump(StartNH, 5),
    EndNH = fct_lump(EndNH, 5)
  ) %>%
  # remove columns that would 'spoil' the prediction
  select(-Duration, -Distance,
         -EndLongitude, -EndLatitude
  ) %>%
  as_tibble()
```

# Split in Train and Test Data

Class imbalance needs to be addressed with stratified sampling, as we did in the earlier post:

```{r}
set.seed(42)

trips_split <- initial_split(trips, strata = EndNH)

trips_train <- training(trips_split)
trips_test <- testing(trips_split)
```

# Define a recipe

Let's define a recipe to address this problem. We shall `themis::step_downsample()` the data, which would achieve two goals:

1. Address class imbalance.
2. Reduce computational workload, given our limited computational resources.

```{r}
trips_recipe <- trips_train %>%
  recipe(EndNH ~ .) %>%
  # problem: step_date does not extract times!
  step_mutate(HourNum = format(strptime(StartTime,'%Y-%m-%d %H:%M:%S'),'%H')) %>%
  # turn it into a factor
  step_string2factor(HourNum) %>%
  # create factors out of StartTime
  step_date(StartTime, features = c('dow', 'month', 'year')) %>%
  # create holiday dummy:
  step_holiday(StartTime, holidays = timeDate::listHolidays("US")) %>%
  # remove StartTime col
  step_rm(StartTime) %>%
  # turn factor-features into binary dummies (i.e. one per column: 1-0):
  step_dummy(all_nominal(), -all_outcomes()) %>%
  # remove predictors with zero variance:
  step_zv(all_predictors()) %>%
  # downsample the data: each class is as numerous as the least represented
  themis::step_downsample(EndNH, under_ratio = 1)
```

# Define a model with parameters to tune

```{r}
tree_to_tune <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tree_to_tune
```

# Add a recipe and the model to a tuning workflow

```{r}
tree_tuning_workflow <-
  workflow() %>%
  add_model(tree_to_tune) %>%
  add_recipe(trips_recipe)

tree_tuning_workflow
```


# Define the tuning grid

```{r}
tree_tune_grid <- dials::grid_regular(cost_complexity(),
                               tree_depth(),
                               levels = 5)

tree_tune_grid
```

# Create the `resamples` via `k-fold` cross validation

Defaults are `v = 10` and `repeats = 1`: we will stick with these, because given the large number of observations our hardware can't deal with more than one repeat. Folds are stratified, to ensure equal representation.

```{r}
set.seed(42)

tree_cv_folds <-
  trips_train %>%
  vfold_cv(strata = "StartNH")

tree_cv_folds
```


# Cross validation for parameter tuning

```{r}
set.seed(42)

tree_fit_resamples <-
  tree_tuning_workflow %>%
  tune_grid(
    resamples = tree_cv_folds,
    grid = tree_tune_grid
  )

tree_fit_resamples
```

